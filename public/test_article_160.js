{"code":0,"message":"","data":{"id":160,"user_id":3,"title":"\u4ece\u96f6\u5f00\u59cb\u57fa\u4e8e JavaScript \u6784\u5efa\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\uff081\uff09","source_url":"https:\/\/hackernoon.com\/neural-networks-from-scratch-for-javascript-linguists-part1-the-perceptron-632a4d1fbad2","figure":"\/upload\/figure\/1495004459850.gif","summary":"\u672c\u6587\u4e0d\u662f\u7eaf\u7cb9\u7684\u524d\u7aef\u5f00\u53d1\u6587\u7ae0\uff0c\u5bf9\u4e8e\u542c\u8bf4\u8fc7\u4eba\u5de5\u667a\u80fd\u4e0e\u795e\u7ecf\u7f51\u7edc\u5e76\u4e14\u6709\u5174\u8da3\u7684\u5f00\u53d1\u8005\u4e0d\u59a8\u4e00\u8bfb\u3002\u800c\u672c\u6587\u5219\u662f\u6e10\u8fdb\u5730\u4ecb\u7ecd\u795e\u7ecf\u7f51\u7edc\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u57fa\u7840\u3001\u5982\u4f55\u4f7f\u7528 JavaScript \u5b9e\u73b0\u7b80\u5355\u7684\u6570\u5b66\u516c\u5f0f\u3001\u5982\u4f55\u5b9e\u73b0\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u7b49\u5185\u5bb9\u3002","author":"Elyx0","author_url":"https:\/\/hackernoon.com\/@Elyx0","content":"<section><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*iXxnl7MpExTHjVpukHPMsg.gif\"\/><\/p><p>Snakes surviving thanks to machine learning:&nbsp;<a href=\"http:\/\/snakeneuralnetwork.herokuapp.com\/\">here<\/a><\/p><p>There is a big chance that you heard about Neural Network and Artificial Intelligence in the course of the previous months, which seem to accomplish wonders, from&nbsp;<a href=\"http:\/\/karpathy.github.io\/2015\/10\/25\/selfie\/\">rating your selfie<\/a>&nbsp;to&nbsp;<a href=\"http:\/\/mashable.com\/2014\/08\/18\/siri-fails\/#hMwSkraMiPqp\">SIRI<\/a>&nbsp;understanding your voice, beating players at games such as Chess and&nbsp;<a href=\"http:\/\/www.cbc.ca\/news\/technology\/go-google-alphago-lee-sedol-deepmind-1.3488913\">Go<\/a>,&nbsp;<a href=\"https:\/\/twitter.com\/goodfellow_ian\/status\/851124988903997440\">turning a horse into a zebra<\/a>&nbsp;or making you look younger, older, or&nbsp;<a href=\"https:\/\/www.faceapp.com\/\">from the other gender<\/a>.<\/p><p>Those keywords have in fact been used so many times in so many different contexts, making this whole \u201cA.I\u201d lexical field so confusing that it does at best correlates with \u201csomething that looks smart\u201d.<\/p><p>Maybe because of the confusion machine learning seems way too complex, way too hard to grasp, like \u201cI bet there is so much math, this is not for me!!\u201d.<\/p><p>Well, don\u2019t worry, it was for me too, so let\u2019s go on a&nbsp;journey, I\u2019ll tell you everything I learned, some misconceptions I had, how to interpret the results, and some basic vocabulary and fun facts along the way.<\/p><h3>What are we talking&nbsp;about?<\/h3><p>Imagine a&nbsp;box&nbsp;in which you carve some holes then you throw in it a&nbsp;predefined&nbsp;amount&nbsp;of numbers,&nbsp;either&nbsp;<code>0<\/code>&nbsp;or&nbsp;<code>1<\/code>.&nbsp;Then the box vibrates violently and from each hole the box spurts out one number:&nbsp;<code>0<\/code>&nbsp;or<code>1<\/code>. Great.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*qlYvCq6YolCNQiqdPtNdng.png\"\/><\/p><p>Now what?<\/p><p>Initially, your box&nbsp;is dumb&nbsp;as hell, it won\u2019t magically give you the result you expect, you have to&nbsp;train&nbsp;it to achieve the&nbsp;goal&nbsp;you want.<\/p><h3>Understanding through&nbsp;history<\/h3><p>My biggest mistake was trying to wrap my head around concepts by just looking at the tip of the iceberg, playing with libraries and getting mad when it didn\u2019t work. You can\u2019t really afford that with neural nets.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*EQfIFkckck1Cj1TNLDXigQ.gif\"\/><\/p><p>Time to go&nbsp;back<\/p><p>The invention of this wonderful cardboard cheese originated around 1943 when the neurophysiologist&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Warren_Sturgis_McCulloch\">Warren Sturgis McCulloch<\/a>, 45 years, and his colleague Walter Pitts wrote a paper named: \u201c<a href=\"http:\/\/www.cs.cmu.edu\/~.\/epxing\/Class\/10715\/reading\/McCulloch.and.Pitts.pdf\">A Logical Calculus of the Ideas Immanent in Nervous Activity<\/a>\u201d.<\/p><p>Pursuing the quest of the classical philosophers of Greece, he attempted to&nbsp;model how the brain works&nbsp;mathematically.<\/p><p>That was indeed pretty bright given since&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Santiago_Ram%C3%B3n_y_Cajal\">how long we knew about the neurons<\/a>&nbsp;(~1900) and that the electric nature of their signals to communicate wouldn\u2019t be demonstrated before the late 1950s.<\/p><p>Let\u2019s take a second here to remember that published papers do not mean that the person who wrote it was absolutely&nbsp;right, it meant this guy:&nbsp;had a hypothesis, knew a bit in this field, had some kind of results, sometimes not applicable and published it.&nbsp;<br\/>Then other qualified people from the field have to try it,&nbsp;reproduce the results&nbsp;and decide whether or not it\u2019s a nice base to build upon.<\/p><p>When left unchecked, and toyed with&nbsp;p-hacking, some papers can give birth to some aberrations like this 2011 paper saying that&nbsp;<a href=\"https:\/\/www.youtube.com\/watch?v=42QuXLucH3Q\">people could see in the future<\/a>.<\/p><p>Nevertheless, we now know that our&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Warren_Sturgis_McCulloch\">McCulloch<\/a>&nbsp;was a good guy and in his paper he tried to model some algorithms after the physical brain, constituted itself of a&nbsp;nervous system&nbsp;(possessed by most multicellular animals), which is actually a net of&nbsp;neurons.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*NORBXIXJ54IckCJDfcc9Og.gif\"\/><\/p><p>Hoping you won\u2019t lose too many of these reading the following. (<a href=\"https:\/\/www.youtube.com\/watch?v=qPix_X-9t7E\">Source<\/a>)<\/p><p>The human brain has ~86 billion&nbsp;of these fellas, each having&nbsp;axons&nbsp;and&nbsp;dendrites and synapses&nbsp;that connect&nbsp;each&nbsp;neuron&nbsp;to&nbsp;~7000&nbsp;others.&nbsp;Which is almost as many connections as the&nbsp;<a href=\"http:\/\/www.esa.int\/Our_Activities\/Space_Science\/Herschel\/How_many_stars_are_there_in_the_Universe\">number of galaxies in the Universe<\/a>.<br\/>&nbsp;<br\/>Since we\u2019re not good at visualizing big numbers, here is how to see $1B:<\/p><p>Now imagine 86 times that<\/p><p>The problem for McCulloch was that the economic context wasn\u2019t thriving in 1943: We\u2019re at war, Franklin D Roosevelt froze the prices, salaries and wages to prevent inflation, Nikola Tesla passed away and the best computer available was the&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/ENIAC\">ENIAC<\/a>, which cost $7 millions, for&nbsp;<a href=\"http:\/\/img04.deviantart.net\/73c7\/i\/2011\/249\/1\/f\/your_mom_by_khelsiieexkhaotika-d4943v7.jpg\">30 tons<\/a>&nbsp;(The sexism in tech ran pretty wild(er?) at that time and the fact that ENIAC was&nbsp;<a href=\"http:\/\/blogs.smithsonianmag.com\/smartnews\/2013\/10\/computer-programming-used-to-be-womens-work\/\">invented by&nbsp;6 women<\/a>&nbsp;caused their male colleagues to wrongly underestimate it).<br\/>As a comparison, a standard Samsung flip phone from 2005 had&nbsp;1300x&nbsp;the ENIAC computing power.<\/p><p>Then in&nbsp;1958,&nbsp;computers were doing a bit better,&nbsp;and&nbsp;Frank Rosenblatt, inspired by McCulloch, gifted us with the&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Perceptron\">Perceptron<\/a>.<\/p><p>Everybody was happy digging deeper until&nbsp;Marvin&nbsp;Minsky,&nbsp;11 years later, decided he&nbsp;<a href=\"http:\/\/blogs.umass.edu\/brain-wars\/the-debates\/minsky-vs-rosenblatt\/\">did not like that idea<\/a>&nbsp;and that Frank Perceptron wasn\u2019t cut for the job, as he explained by publishing a book in which he said: \u201c<em>Most of Rosenblatt writing&nbsp;\u2026 is without scientific value\u2026<\/em>\u201d the impact of this book is that it drained the already low funding in this field.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*HSUPKzfZtlBpAgvkQZ752w.gif\"\/><\/p><p>Classic Minsky.<\/p><p>What did Minsky have against McCulloch padawan?<\/p><h4>The linearity conundrum<\/h4><p>While this may sound like a&nbsp;<em>Big Bang Theory<\/em>&nbsp;episode title, it actually represents the basis of Minsky theory to detract from the original Perceptron.<\/p><p>Rosenblatt perceptron looks very similar to our box, in which this time we&nbsp;drilled a single hole:<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*Bj6CgH_GSUsBCRpS_N6XiQ.png\"\/><\/p><p>A Neural Network can actually take inputs&nbsp;between&nbsp;0 and&nbsp;1<\/p><p>If the sum of our&nbsp;inputs signals(x1\u2026x4)&nbsp;multiplied by their respective&nbsp;weights (w1\u2026w4)&nbsp;plus the&nbsp;bias (b)&nbsp;are enough to make the&nbsp;result&nbsp;gate go above the&nbsp;threshold (T), our door will liberate the value&nbsp;<code>1<\/code>otherwise,&nbsp;<code>0<\/code>.<\/p><p>For this to happen the threshold value is compared to the result of the&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Activation_function\">activation function<\/a>.&nbsp;Exactly like brain neurons respond to a&nbsp;stimulus. If the stimulus is too low the neuron doesn\u2019t fire the signal along the axon to the dendrites.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*yMBxDghV-iI7M9ABmXeY1g.gif\"\/><\/p><p>Depolarization of a neuron thanks to the magic of the&nbsp;<a href=\"https:\/\/www.youtube.com\/watch?v=OZG8M_ldA1M\">sodium-potassium pump<\/a><\/p><p>Code wise it globally looks like this:<\/p><pre>\/\/&nbsp;Defining&nbsp;the&nbsp;inputs\r\nconst&nbsp;x1&nbsp;=&nbsp;1;\r\nconst&nbsp;x2&nbsp;=&nbsp;0.3;\r\nconst&nbsp;x3&nbsp;=&nbsp;0.2;\r\nconst&nbsp;x4&nbsp;=&nbsp;0.5;<\/pre><pre>\/\/&nbsp;Defining&nbsp;the&nbsp;weights\r\nconst&nbsp;w1&nbsp;=&nbsp;1.5;\r\nconst&nbsp;w2&nbsp;=&nbsp;0.2;\r\nconst&nbsp;w3&nbsp;=&nbsp;1.1;\r\nconst&nbsp;w4&nbsp;=&nbsp;1.05;<\/pre><pre>const&nbsp;Threshold&nbsp;=&nbsp;1;\r\nconst&nbsp;bias&nbsp;=&nbsp;0.3;<\/pre><pre>\/\/&nbsp;The&nbsp;value&nbsp;evaluated&nbsp;against&nbsp;the&nbsp;threshold&nbsp;is&nbsp;the&nbsp;sum&nbsp;of&nbsp;the\r\n\/\/&nbsp;inputs&nbsp;multiplied&nbsp;by&nbsp;their&nbsp;weights\r\n\/\/&nbsp;(1*1.5)+(.3*0.2)+(.2*1.1)+(.5*1.05)<\/pre><pre>const&nbsp;sumInputsWeights&nbsp;=&nbsp;x1*w1&nbsp;+&nbsp;x2*w2&nbsp;+&nbsp;x3*w3&nbsp;+&nbsp;x4*w4;&nbsp;\/\/&nbsp;2.305\r\nconst&nbsp;doorWillOpen&nbsp;=&nbsp;activation(sumInputsWeights&nbsp;+&nbsp;bias)&nbsp;&gt;&nbsp;Threshold;&nbsp;\/\/&nbsp;true<\/pre><p>In the human body a neuron electrical off state is at -70mV and its activation threshold is when it hits -55mV.<\/p><p>In the case of the original Perceptron, this activation is handled by the&nbsp;Heaviside step function.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*sG_Yqs9TH-v8gEToNU0LoA.png\"\/><\/p><p>0&nbsp;if x is negative,&nbsp;1&nbsp;if x is null or positive. x being the sumInputsWeights+bias.<\/p><p>One of the most known activation function is the&nbsp;sigmoid function:<br\/>&nbsp;<code>f(x) = 1 \/ (1 + exp(-x))<\/code>&nbsp;and the&nbsp;bias&nbsp;is generally used to&nbsp;shift&nbsp;its activation threshold:<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*KVPmkQUZrWKe8loTfwMwyA.png\"\/><\/p><p>Tweaking the activation function can yield to adjusted results altering when the neuron&nbsp;fires<\/p><p>Some activations function allow negative values as output, some don\u2019t. This will prove itself important when using results of the activation function of one perceptron to feed it as the inputs of another perceptron.<\/p><p>Using&nbsp;<code>0<\/code>&nbsp;as an input always silences its associated weight, causing its connection to be non-existent in the sum.<\/p><p>So instead of having the possibility of weighting it down, it acts like an abstention vote. Whereas sometimes we might want<code>0<\/code>as an input to be a vote for the opposite candidate.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*dq-QWOg88oDkgMKBeCujTg.png\"\/><\/p><p>Many other activations functions&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Activation_function\">exist<\/a><\/p><p>The Perceptron is known as a&nbsp;binary classifier&nbsp;meaning&nbsp;it can only classify between&nbsp;2&nbsp;options (Spam vs Not Spam, Oranges vs Not-Oranges\u2026 etc)<\/p><p>It\u2019s also designated as a&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Linear_classifier\">linear classifier<\/a>&nbsp;meaning&nbsp;its goal is to&nbsp;identify to which class an object belongs&nbsp;to according to its&nbsp;characteristics&nbsp;(or \u201cfeatures\u201d: our&nbsp;x1 to x4))&nbsp;by iterating until it finds&nbsp;a SINGLE line&nbsp;that correctly&nbsp;separates&nbsp;the entities from each class.<\/p><p>We give our classifier some examples of expected results given some inputs, and it will&nbsp;train itself&nbsp;to find this separation, by adjusting the&nbsp;weights&nbsp;assigned&nbsp;to every input and its&nbsp;bias.<\/p><p>As an example let\u2019s classify some entities between \u201cFriendly or not\u201d according to 2 characteristics:&nbsp;Teeth&nbsp;and&nbsp;Size&nbsp;using a perceptron<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*QEwusFfiilfoeeNNLXFygg.gif\"\/><\/p><p>Depending on the sources the cat seems very borderline indeed<\/p><p>Now that we trained our perceptron, we can predict samples it\u2019s never seen before:<\/p><p><em>Head&nbsp;<\/em><a href=\"https:\/\/rosenblattperceptron.herokuapp.com\/\"><em>here<\/em><\/a><em>&nbsp;for the live&nbsp;<\/em><a href=\"https:\/\/rosenblattperceptron.herokuapp.com\/\"><em>demo<\/em><\/a><em>. No seriously, check it.<\/em><\/p><p>What Minsky reproached to Rosenblatt was the following: what if suddenly my training set contained a&nbsp;giant snake, with almost no teeth but quite as big as an elephant.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*izrfOXRu3B1dL2Ys5fBZkg.png\"\/><\/p><p>The training set is not separable by one line&nbsp;anymore<\/p><p>It now requires&nbsp;2&nbsp;lines to correctly separate the red entities from the green ones. Using a single perceptron this impossibility would cause the perceptron to try and run forever, unable to classify with a&nbsp;single straightline.<\/p><p>By this time you should be able to handle the usual representation of a perceptron:<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*K_-erXb47r6CeoJU61Yg6w.png\"\/><\/p><p>The bias can be applied either after the sum or as an added weight for a fictional input being always&nbsp;1<\/p><h4>Solving the bilinear&nbsp;issue:<\/h4><p>One way to solve this is to simply train 2 perceptrons one responsible for the top left separation, the other for the bottom right, and we&nbsp;plug them together&nbsp;with an&nbsp;and&nbsp;rule, creating some kind of&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Perceptron#Multiclass_perceptron\">Multiclass perceptron<\/a>.<\/p><pre>const&nbsp;p1&nbsp;=&nbsp;new&nbsp;Perceptron();\r\np1.train(trainingSetTopLeft);\r\np1.learn();&nbsp;\/\/&nbsp;P1&nbsp;ready.<\/pre><pre>const&nbsp;p2&nbsp;=&nbsp;new&nbsp;Perceptron();\r\np2.train(trainingSetBottomRight);\r\np2.learn();&nbsp;\/\/&nbsp;P2&nbsp;ready.<\/pre><pre>const&nbsp;inputs&nbsp;=&nbsp;[x1,x2];<\/pre><pre>const&nbsp;result&nbsp;=&nbsp;p1.predict(inputs)&nbsp;&amp;&nbsp;p2.predict(inputs);<\/pre><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*1WgDiZ2uR1H0JWFznDuLaQ.png\"\/><\/p><p>There are other ways to solve this that we\u2019ll explore in part&nbsp;2<\/p><p>The AND operation aka A^B is part of the logical operations a perceptron can perform, by putting&nbsp;w5&nbsp;and&nbsp;w6&nbsp;at around&nbsp;<code>0.6<\/code>&nbsp;and applying a bias of&nbsp;<code>-1<\/code>&nbsp;to our sum, we indeed created an AND predictor.&nbsp;<br\/>Remember that the inputs connected with w5 and w6 are coming from the activation of the Heaviside step function, yielding only&nbsp;<code>0<\/code>&nbsp;or&nbsp;<code>1<\/code>&nbsp;as output.<\/p><pre>For&nbsp;0&nbsp;&amp;&nbsp;0&nbsp;:&nbsp;(0*0.6&nbsp;+&nbsp;0*0.6)-1&nbsp;is&nbsp;&lt;0,&nbsp;Output:&nbsp;0\r\nFor&nbsp;0&nbsp;&amp;&nbsp;1&nbsp;:&nbsp;(0*0.6&nbsp;+&nbsp;1*0.6)-1&nbsp;is&nbsp;&lt;0,&nbsp;Output:&nbsp;0\r\nFor&nbsp;1&nbsp;&amp;&nbsp;1&nbsp;:&nbsp;(1*0.6&nbsp;+&nbsp;1*0.6)-1&nbsp;is&nbsp;&gt;0,&nbsp;Output:&nbsp;1<\/pre><p>By doing this chaining and having \u201celongated\u201d our original perceptron we have created what is called a&nbsp;hidden layer,&nbsp;which is basically some neurons plugged inbetween the original inputs, and the final output.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*FibRm8jGbHO8hlwGOyAmBQ.png\"\/><\/p><p>They should really be called \u201c<a href=\"https:\/\/www.cs.cmu.edu\/~dst\/pubs\/byte-hiddenlayer-1989.pdf\">feature detectors layers<\/a>\u201d or \u201csub problem&nbsp;layers\u201d<\/p><p>The worst part is that&nbsp;Minsky knew all this&nbsp;and still decided to focus on the simplest version of the perceptron to spit on all of Rosenblatt\u2019s work. Rosenblatt had already covered the multi-layer &amp; cross-coupled perceptron in his own book&nbsp;<em>Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms&nbsp;<\/em>in 1962.<\/p><p>Which is a bit like publishing a book named&nbsp;<em>Transistors,&nbsp;<\/em>express that a single one of them is worthless and never acknowledge the computer.<\/p><h4>But where is the perceptron line coming&nbsp;from?<\/h4><p>You should all be familiar with the following equations:&nbsp;<code>y=f(x)<br\/>y = mx + c<\/code>&nbsp;or&nbsp;<code>y = ax + b<\/code>&nbsp;.&nbsp;<br\/>So how do we find this&nbsp;<code>m<\/code>&nbsp;and&nbsp;<code>c<\/code>&nbsp;from our trained perceptron?<\/p><p>To figure this out we have to remember the original equation of our perceptron:&nbsp;<code>x1*w1 + x2*w2 + bias &gt; T<\/code><\/p><p>Bias and Threshold are the&nbsp;<a href=\"http:\/\/stackoverflow.com\/questions\/16609310\/in-neural-networks-does-a-bias-change-the-threshold-of-an-activation-function\">same concepts<\/a>, and for the perceptron&nbsp;<code>T = 0<\/code>which means the equation becomes&nbsp;<code>x1*w1 +x2*w2 &gt; -bias<\/code>&nbsp;which can be rewritten as:&nbsp;<br\/><code>x2 &gt; (-w1\/w2)*x1 + (-bias\/w2)<\/code>&nbsp;comparing this to:&nbsp;<br\/><code>y = m*x + b<\/code>&nbsp;we can see that<\/p><p><code>y<\/code>stands for&nbsp;<code>x2<br\/>x<\/code>for&nbsp;<code>x1<br\/>m<\/code>&nbsp;for<code>(-w1\/w2)<\/code>&nbsp;and&nbsp;<code>b<\/code>&nbsp;for&nbsp;<code>(-bias\/w2)<\/code><\/p><p>Which means the&nbsp;gradient (m)&nbsp;of our line is determined by the 2 weights, and the&nbsp;place where the line cuts the vertical axis&nbsp;is determined by the bias and the 2nd weight.<\/p><p>We can now simply select two&nbsp;<code>x<\/code>&nbsp;values (0 and 1), replace them in the line equation and find the corresponding&nbsp;<code>y<\/code>&nbsp;values, then trace a line between the two points.&nbsp;<code>(0;f(0))<\/code>and&nbsp;<code>(1;f(1))<\/code>&nbsp;.<\/p><p>Like a good crime scene having access to the equation allows us to make some observations:<\/p><pre>y&nbsp;=&nbsp;(-w1\/w2)x&nbsp;+&nbsp;(-bias\/w2)<\/pre><p>\u00b7 The&nbsp;gradient (steepness)&nbsp;of the line depends only on the two weights<br\/>\u00b7 The&nbsp;minus&nbsp;sign in front of&nbsp;<code>w1<\/code>&nbsp;means that if both weights have the same sign, the line will slope down like a&nbsp;<code>\\<\/code>, if they are exclusively different it will slope up&nbsp;<code>\/<br\/><\/code>\u00b7 Adjusting&nbsp;w1&nbsp;will affect the steepness but not where it intersects the vertical axis,&nbsp;w2 instead&nbsp;will have an effect on&nbsp;both<br\/>\u00b7 Because the&nbsp;bias is the numerator&nbsp;(top half of the fraction) increasing it will push the line higher up the graph. (will cut the vertical axis at a higher point)<\/p><p>You can check the final weights in the console of the&nbsp;<a href=\"https:\/\/rosenblattperceptron.herokuapp.com\/\">demo<\/a>&nbsp;by entering&nbsp;<code>app.perceptron.weights<\/code><\/p><p>And that is exactly what our perceptron tries to optimize until it finds the correct line<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*1hQqAC67zmS9nomTuxa7Uw.gif\"\/><\/p><p>I know it might look like 2 lines but it\u2019s really one moving super&nbsp;fast<\/p><h4>What do you mean by \u201coptimize\u201d&nbsp;?<\/h4><p>As you can guess, our perceptron can\u2019t blindly guess and try every value for its weights until it finds a line that correctly separates the entities. What we have to apply is the&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Delta_rule\">delta rule<\/a>.<\/p><p>It\u2019s a learning rule for updating the weights associated with the inputs in a single layer neural network, and is represented by the following:<\/p><p><img class=\"graf-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*Ba8c5NlO8VkCu9t9go-28Q.png\"\/><\/p><p>Oh god&nbsp;math!<\/p><p>Don\u2019t worry it can be \u201csimplified\u201d like this:<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*7BCDXvKT7TCVhqeBhWfinA.png\"\/><\/p><p>We do this for every item in the training&nbsp;set.<\/p><p>Doing&nbsp;<code>expected\u200a\u2014\u200aactual<\/code>&nbsp;represents an&nbsp;error&nbsp;value (or cost). The goal is to iterate through the training set and reduce this error\/cost to the minimum, by adding or subtracting a small amount to the weights of each input until it validates all the training set expectations.<\/p><p>If after some iterations the&nbsp;error&nbsp;is&nbsp;<code>0<\/code>&nbsp;for every item in the training set&nbsp;. Our perceptron is trained, and our line equation using these final weights correctly separates in two.<\/p><h4>Beware of the learning&nbsp;rate<\/h4><p>In the equation above,&nbsp;\u03b1&nbsp;represents a constant: the learning rate, that will have an impact on how much each weight gets altered.<\/p><p>\u00b7 If&nbsp;\u03b1&nbsp;is too&nbsp;small,&nbsp;it will require&nbsp;more iterations than needed&nbsp;to find the correct weights and you might get trapped in a local minima.&nbsp;<br\/>\u00b7 If&nbsp;\u03b1&nbsp;is too&nbsp;big&nbsp;the learning might&nbsp;never find&nbsp;some&nbsp;correct weights.<\/p><p>One way to see it is imagining a poor guy with<a href=\"https:\/\/i.ytimg.com\/vi\/qHuhZxyWiFA\/hqdefault.jpg\">&nbsp;metal boots<\/a>&nbsp;tied together that wants to reach a treasure at the bottom of a cliff but he can only move by jumping by&nbsp;\u03b1&nbsp;meters:<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*c__J4PZOhc6NV5W7hv3R4w.png\"\/><\/p><p>\u03b1&nbsp;too&nbsp;big<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*5LUFC0ug0FEOB0HPJBuTcA.png\"\/><\/p><p>\u03b1&nbsp;too&nbsp;small<\/p><p>One thing you might want to do when reading an article on Wikipedia is to head on the \u201c<em>Talk<\/em>\u201d section which discusses disputed areas of the content.<\/p><p><img class=\"progressiveMedia-image js-progressiveMedia-image\" src=\"https:\/\/cdn-images-1.medium.com\/max\/800\/1*hHnczTt0uvp4g6prYefCNA.png\"\/><\/p><p>In the case of the delta formula, the&nbsp;<em>content<\/em>&nbsp;said that it couldn\u2019t be applied to the perceptron because the Heaviside derivative does not exist at&nbsp;<code>0<\/code>&nbsp;but the&nbsp;<em>Talk section<\/em>&nbsp;provided articles of M.I.T teachers using it.<\/p><p>By putting everything we learned we finally code a perceptron:<\/p><p>Can\u2019t be used as is. Actual code&nbsp;<a href=\"https:\/\/github.com\/Elyx0\/rosenblattperceptronjs\/blob\/master\/src\/Perceptron.js\">here<\/a><\/p><\/section><p><\/p><section><hr\/><p>Additionally, the Perceptron goes into the&nbsp;feedforward neural network&nbsp;category which is just fancy wording for saying that the connections between the units do not form a cycle.<\/p><p>To cut some slack to Minsky, while the perceptron algorithm is guaranteed to converge on&nbsp;<em>some<\/em>&nbsp;solution in the case of a linearly separable training set, it may still pick&nbsp;<em>any valid&nbsp;<\/em>solution and some problems may admit many solutions of varying quality.<\/p><p>Your brain does a bit of the same as soon as it finds a correct way to talk to a muscle (ie: the muscle responds correctly,) it will settle for it. This brain-to-muscle code is different for everyone!<\/p><p>Intellectuals like to argue with each other all the time like Minksy and Rosenblatt did. Even&nbsp;Einstein&nbsp;was proven wrong in his fight against&nbsp;Niels Bohr&nbsp;on&nbsp;<a href=\"https:\/\/en.wikipedia.org\/wiki\/Bohr%E2%80%93Einstein_debates\">quantum indeterminism<\/a>, arguing with his famous quote that: \u201c<em>God does not play dice with the universe<\/em>\u201d.<\/p><p>We\u2019ll end with some poetry from the father of the neural networks our dear Warren McCulloch (<em>he&nbsp;<\/em><a href=\"https:\/\/en.wikipedia.org\/wiki\/Warren_Sturgis_McCulloch#Biography\"><em>really<\/em><\/a><em>&nbsp;was a poet<\/em>).<br\/>I hope you learned some things and I will see you soon for the Part 2.<\/p><\/section><p><\/p><p><br\/><\/p>","click_count":451,"status":"publish","likes":0,"dislikes":0,"created_at":"2017-05-17 15:00:59","updated_at":"2017-05-17 15:01:42","deleted_at":null,"team_id":null}}